{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7308df8f",
   "metadata": {},
   "source": [
    "### Semantic Kernel Introduction\n",
    "\n",
    "#### Overview of Semantic Kernel (SK) and Its Importance\n",
    "***Semantic Kernel*** is an open-source SDK from Microsoft that acts as middleware between your application code and AI large language models (LLMs). It enables developers to easily integrate AI into apps by letting AI agents call code functions and by orchestrating complex tasks. SK is lightweight and modular, designed for enterprise-grade solutions with features like telemetry and filters for responsible AI. Major companies (including Microsoft) leverage SK because it’s flexible and future-proof – you can swap in new AI models as they emerge without rewriting your code. In short, SK helps build robust, scalable AI applications that can evolve with advancing AI capabilities.\n",
    "\n",
    "Key reasons why Semantic Kernel is important for AI application development:\n",
    "\n",
    "- Bridging AI and Code: **SK combines natural language prompts with your existing code and APIs**, allowing AI to take actions. The AI can request a function call and SK will execute that function and return results back to the model. This bridges the gap between what the AI intends and what your code can do.\n",
    "- **Plugins (Skills)**: You can expose functionalities (from simple math to complex business logic or external APIs) as SK plugins. By describing your code to the AI (via function definitions), the model can invoke these functions to fulfill user requests. This plugin architecture makes your AI solutions modular and extensible.\n",
    "- **Enterprise-ready**: SK includes support for security, observability, and compliance (e.g. integration with Azure services, monitoring, content filtering). Hooks and filters ensure you can enforce policies (for instance, prevent sensitive data leakage).\n",
    "- **Multi-modal & Future-Proof**: SK natively supports multiple AI services (OpenAI, Azure OpenAI, HuggingFace, etc.) and modalities. Chat-based APIs can be extended to voice or other modes. As new models (like vision-enabled models or better language models) come out, SK lets you plug them in without major changes.\n",
    "- **Rapid Development**: By handling the heavy lifting of prompt orchestration, function calling, and memory management, SK enables faster development of AI features. You focus on defining what you want the AI to do (skills, prompts) and SK handles how to do it. Microsoft claims that SK helps “deliver AI solutions faster than any other SDK” due to its ability to automatically call functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f19c8",
   "metadata": {},
   "source": [
    "### Services and Core Components of SK\n",
    "Semantic Kernel's architecture revolves around a few core components and services:\n",
    "\n",
    "- **Kernel**: The central object that orchestrates everything. The Kernel holds configuration for AI services, manages plugins (skills), coordinates function calls, and maintains contextual state (memory). You typically create one Kernel instance in your app and use it to register functions and perform AI queries.\n",
    "- **AI Services**: SK connects to AI models for different tasks:\n",
    "Chat Models: e.g. Azure OpenAI GPT-4o-mini or GPT-4o for natural language generation and understanding.\n",
    "Embedding Models: for converting text to vector embeddings (used in memory/search).\n",
    "Other Modalities: connectors for images, speech, etc., if needed.\n",
    "\n",
    "Semantic Kernel can automatically read your .env (automatically read from root of project) to access Azure OpenAI using the following variables:\n",
    "\n",
    "    AZURE_OPENAI_ENDPOINT - The endpoint it should talk to by default\n",
    "    AZURE_OPENAI_API_KEY - The API Key it should use\n",
    "    AZURE_OPENAI_API_VERSION - Inference API version it should use per default\n",
    "    AZURE_OPENAI_CHAT_DEPLOYMENT_NAME - Model deployment name it should use per default\n",
    "\n",
    "Before you continue, make sure your .env (copied from .env.sample) is filled out correctly.\n",
    "\n",
    "You configure the Kernel with the endpoints/keys for the services you need. For example, adding an Azure OpenAI chat completion service as follows ->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8562d7aa",
   "metadata": {},
   "source": [
    "#### 1. Create a kernel and add a chat completion model to it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f940329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "kernel = Kernel()\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Auto-loads defaults from .env file, alternatively you can set endpoint, deployment_name and api_key directly\n",
    "chat_completion=AzureChatCompletion()\n",
    "\n",
    "kernel.add_service(chat_completion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8250a28d",
   "metadata": {},
   "source": [
    "### 2. Provide the kernel with the required skills. These can be defined as \"functions\" and \"plugins\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981a4b5",
   "metadata": {},
   "source": [
    "##### Define function via a prompt (natural language function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8188f41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Kernel is a tiny, open box of tools that helps you make smart robot friends and lets you put the coolest AI brains into your own projects really easily!\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"{{$input}}\\n\\ntranslate to {{$target_language}} and change the tone to {{$target_tone}}:\"\n",
    "\n",
    "rewrite_expert = kernel.add_function(\n",
    "    prompt=prompt_template,\n",
    "    function_name=\"toner\",\n",
    "    plugin_name=\"Toner\"\n",
    ")\n",
    "\n",
    "# Use the function\n",
    "sample_text = \"\"\"\n",
    "Semantic Kernel is a lightweight, open-source development kit that lets \n",
    "you easily build AI agents and integrate the latest AI models into your codebase\n",
    "\"\"\"\n",
    "\n",
    "# async function call need to be awaited\n",
    "summary = await kernel.invoke(rewrite_expert, input=sample_text, target_language = \"english\", target_tone=\"childish\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab0ed94",
   "metadata": {},
   "source": [
    "#### Define function via code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "401931bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from semantic_kernel.functions import kernel_function\n",
    "# Define the LightModel and LightsPlugin classes\n",
    "\n",
    "class LightModel(TypedDict):\n",
    "    id: int\n",
    "    name: str\n",
    "    is_on: bool | None\n",
    "    brightness: int | None\n",
    "    hex: str | None\n",
    "\n",
    "\n",
    "class LightsPlugin:\n",
    "    def __init__(self, lights: list[LightModel]):\n",
    "        self.lights = lights\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_lights(self) -> List[LightModel]:\n",
    "        \"\"\"Gets a list of lights and their current state.\"\"\"\n",
    "        return self.lights\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_state(\n",
    "        self, id: Annotated[int, \"The ID of the light\"]\n",
    "    ) -> Optional[LightModel]:\n",
    "        \"\"\"Gets the state of a particular light.\"\"\"\n",
    "        for light in self.lights:\n",
    "            if light[\"id\"] == id:\n",
    "                return light\n",
    "        return None\n",
    "\n",
    "    @kernel_function\n",
    "    async def change_state(\n",
    "        self, id: Annotated[int, \"The ID of the light\"], new_state: LightModel\n",
    "    ) -> Optional[LightModel]:\n",
    "        \"\"\"Changes the state of the light.\"\"\"\n",
    "        for light in self.lights:\n",
    "            if light[\"id\"] == id:\n",
    "                light[\"is_on\"] = new_state.get(\"is_on\", light[\"is_on\"])\n",
    "                light[\"brightness\"] = new_state.get(\"brightness\", light[\"brightness\"])\n",
    "                light[\"hex\"] = new_state.get(\"hex\", light[\"hex\"])\n",
    "                return light\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21c916ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelPlugin(name='Lights', description=None, functions={'change_state': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='change_state', plugin_name='Lights', description='Changes the state of the light.', parameters=[KernelParameterMetadata(name='id', description='The ID of the light', default_value=None, type_='int', is_required=True, type_object=<class 'int'>, schema_data={'type': 'integer', 'description': 'The ID of the light'}, include_in_function_choices=True), KernelParameterMetadata(name='new_state', description=None, default_value=None, type_='LightModel', is_required=True, type_object=<class '__main__.LightModel'>, schema_data={'type': 'object', 'properties': {'id': {'type': 'integer'}, 'name': {'type': 'string'}, 'is_on': {'type': ['boolean', 'null']}, 'brightness': {'type': ['integer', 'null']}, 'hex': {'type': ['string', 'null']}}, 'required': ['id', 'name']}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='LightModel', is_required=False, type_object=<class '__main__.LightModel'>, schema_data={'type': 'object', 'properties': {'id': {'type': 'integer'}, 'name': {'type': 'string'}, 'is_on': {'type': ['boolean', 'null']}, 'brightness': {'type': ['integer', 'null']}, 'hex': {'type': ['string', 'null']}}, 'required': ['id', 'name']}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000022725C53FD0>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000022725C60110>, method=<bound method LightsPlugin.change_state of <__main__.LightsPlugin object at 0x0000022725C3C350>>, stream_method=None), 'get_lights': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_lights', plugin_name='Lights', description='Gets a list of lights and their current state.', parameters=[], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='list[LightModel]', is_required=True, type_object=<class 'list'>, schema_data={'type': 'array'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000022725C60250>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000022725C60350>, method=<bound method LightsPlugin.get_lights of <__main__.LightsPlugin object at 0x0000022725C3C350>>, stream_method=None), 'get_state': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_state', plugin_name='Lights', description='Gets the state of a particular light.', parameters=[KernelParameterMetadata(name='id', description='The ID of the light', default_value=None, type_='int', is_required=True, type_object=<class 'int'>, schema_data={'type': 'integer', 'description': 'The ID of the light'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='LightModel', is_required=False, type_object=<class '__main__.LightModel'>, schema_data={'type': 'object', 'properties': {'id': {'type': 'integer'}, 'name': {'type': 'string'}, 'is_on': {'type': ['boolean', 'null']}, 'brightness': {'type': ['integer', 'null']}, 'hex': {'type': ['string', 'null']}}, 'required': ['id', 'name']}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000022725C60390>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000022725C60950>, method=<bound method LightsPlugin.get_state of <__main__.LightsPlugin object at 0x0000022725C3C350>>, stream_method=None)})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dependencies for the plugin\n",
    "# Example with toy data in memory\n",
    "from semantic_kernel.kernel import Kernel\n",
    "\n",
    "lights = [\n",
    "    {\"id\": 1, \"name\": \"Table Lamp\", \"is_on\": False, \"brightness\": 100, \"hex\": \"FF0000\"},\n",
    "    {\"id\": 2, \"name\": \"Porch light\", \"is_on\": False, \"brightness\": 50, \"hex\": \"00FF00\"},\n",
    "    {\"id\": 3, \"name\": \"Chandelier\", \"is_on\": True, \"brightness\": 75, \"hex\": \"0000FF\"},\n",
    "]\n",
    "\n",
    "plugin = LightsPlugin(lights=lights)\n",
    "kernel = Kernel()\n",
    "kernel.add_plugin(\n",
    "    plugin=plugin,\n",
    "    plugin_name=\"Lights\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d2a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 10:48:32,617 - INFO - HTTP Request: POST https://mersaoai.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 10:48:32,620 - INFO - OpenAI usage: CompletionUsage(completion_tokens=14, prompt_tokens=138, total_tokens=152, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "2025-06-10 10:48:32,621 - INFO - processing 1 tool calls in parallel.\n",
      "2025-06-10 10:48:32,622 - INFO - Calling Lights-get_lights function with args: {}\n",
      "2025-06-10 10:48:32,622 - INFO - Function Lights-get_lights invoking.\n",
      "2025-06-10 10:48:32,623 - INFO - Function Lights-get_lights succeeded.\n",
      "2025-06-10 10:48:32,624 - INFO - Function completed. Duration: 0.000578s\n",
      "2025-06-10 10:48:33,165 - INFO - HTTP Request: POST https://mersaoai.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 10:48:33,168 - INFO - OpenAI usage: CompletionUsage(completion_tokens=34, prompt_tokens=263, total_tokens=297, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "2025-06-10 10:48:33,169 - INFO - processing 1 tool calls in parallel.\n",
      "2025-06-10 10:48:33,170 - INFO - Calling Lights-change_state function with args: {\"id\":1,\"new_state\":{\"id\":1,\"name\":\"Table Lamp\",\"is_on\":true}}\n",
      "2025-06-10 10:48:33,171 - INFO - Function Lights-change_state invoking.\n",
      "2025-06-10 10:48:33,171 - INFO - Function Lights-change_state succeeded.\n",
      "2025-06-10 10:48:33,172 - INFO - Function completed. Duration: 0.000796s\n",
      "2025-06-10 10:48:33,579 - INFO - HTTP Request: POST https://mersaoai.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 10:48:33,582 - INFO - OpenAI usage: CompletionUsage(completion_tokens=19, prompt_tokens=338, total_tokens=357, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > The Table Lamp is now turned on. Let me know if you need anything else!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 10:48:40,393 - INFO - HTTP Request: POST https://mersaoai.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 10:48:40,393 - INFO - OpenAI usage: CompletionUsage(completion_tokens=118, prompt_tokens=383, total_tokens=501, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "2025-06-10 10:48:40,393 - INFO - processing 3 tool calls in parallel.\n",
      "2025-06-10 10:48:40,398 - INFO - Calling Lights-change_state function with args: {\"id\": 1, \"new_state\": {\"id\": 1, \"name\": \"Table Lamp\", \"is_on\": false}}\n",
      "2025-06-10 10:48:40,399 - INFO - Function Lights-change_state invoking.\n",
      "2025-06-10 10:48:40,400 - INFO - Function Lights-change_state succeeded.\n",
      "2025-06-10 10:48:40,401 - INFO - Function completed. Duration: 0.000827s\n",
      "2025-06-10 10:48:40,401 - INFO - Calling Lights-change_state function with args: {\"id\": 2, \"new_state\": {\"id\": 2, \"name\": \"Porch light\", \"is_on\": false}}\n",
      "2025-06-10 10:48:40,402 - INFO - Function Lights-change_state invoking.\n",
      "2025-06-10 10:48:40,403 - INFO - Function Lights-change_state succeeded.\n",
      "2025-06-10 10:48:40,403 - INFO - Function completed. Duration: 0.000899s\n",
      "2025-06-10 10:48:40,403 - INFO - Calling Lights-change_state function with args: {\"id\": 3, \"new_state\": {\"id\": 3, \"name\": \"Chandelier\", \"is_on\": false}}\n",
      "2025-06-10 10:48:40,403 - INFO - Function Lights-change_state invoking.\n",
      "2025-06-10 10:48:40,403 - INFO - Function Lights-change_state succeeded.\n",
      "2025-06-10 10:48:40,403 - INFO - Function completed. Duration: 0.000393s\n",
      "2025-06-10 10:48:41,299 - INFO - HTTP Request: POST https://mersaoai.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 10:48:41,306 - INFO - OpenAI usage: CompletionUsage(completion_tokens=82, prompt_tokens=616, total_tokens=698, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > All the lamps are now turned off. Here is the final state of all the lights:\n",
      "\n",
      "1. Table Lamp: Off, Brightness 100, Color FF0000\n",
      "2. Porch light: Off, Brightness 50, Color 00FF00\n",
      "3. Chandelier: Off, Brightness 75, Color 0000FF\n",
      "\n",
      "Let me know if you need anything else!\n",
      "Exiting the chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import (\n",
    "    FunctionChoiceBehavior,\n",
    ")\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "    AzureChatPromptExecutionSettings)\n",
    "import logging\n",
    "\n",
    "# Enable  logging\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "# )\n",
    "\n",
    "# Enable planning\n",
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Create a history of the conversation\n",
    "history = ChatHistory()\n",
    "user_message = \"Please turn on the lamp\"\n",
    "history.add_user_message(user_message)\n",
    "\n",
    "while(True):\n",
    "    # Get the response from the AI\n",
    "    \n",
    "    result = await chat_completion.get_chat_message_content(\n",
    "        chat_history=history,\n",
    "        settings=execution_settings,\n",
    "        kernel=kernel,\n",
    "    )\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Assistant > \" + str(result))\n",
    "    if(user_message.lower() == \"exit\"):\n",
    "        print(\"Exiting the chat. Goodbye!\")\n",
    "        break\n",
    "    # Add the message from the agent to the chat history\n",
    "    history.add_message(result)\n",
    "\n",
    "    # Get user input for the next message\n",
    "    user_message = input(\"Type another request or say 'exit' to end the chat > \")\n",
    "    history.add_user_message(user_message)\n",
    "    if( user_message.lower() == \"exit\"):\n",
    "        #autiomatic function calling\n",
    "        history.add_user_message(\"Please turn off all the lamps and give me final state of all the lights\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6533710c",
   "metadata": {},
   "source": [
    "### A real-world-like usecase: Contoso, Ltd. \n",
    "Let's assume that **Contoso, Ltd.** is an electronic devices distribution company that sells a large variety of devices (ex. headphones, laptops, TVs, etc.) new and refurbished.\n",
    "\n",
    "##### First, let's set up **the operational** database for Contoso, Ltd.\n",
    "- You need an Azure postgres db \n",
    "- Rename env.sample to .env and add credentials to .env file\n",
    "- You can run python db_inti.py to set up the database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c95fe8",
   "metadata": {},
   "source": [
    "#### **Example 1**: an inventory analyst working at the sales departments, wants to perform below actions based on current inventory and sales data of the company:\n",
    "- Get the most updated inventory for a given product name or id\n",
    "- A list of most sold product names for a given categroy\n",
    "- Add a new product to the database\n",
    "\n",
    "This data is stored in a postgreSQL database.\n",
    "\n",
    "In this example, we take a simple approach as follows:\n",
    "1. **securely** Connect to the database\n",
    "2. Load data from required tables from the database when needed\n",
    "3. Provide all necessary functions and plugins\n",
    "4. Develop a code using semantic kernel to get the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52883df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection uri was rertieved successfully.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from src.get_conn import get_connection_uri\n",
    "from pandas import DataFrame\n",
    "\n",
    "conn_uri = get_connection_uri()\n",
    "\n",
    "class Contoso_DataPlugin:\n",
    "    def __init__(self, db_uri: str):\n",
    "        self.conn = psycopg2.connect(db_uri)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        print(\"Connected to company's database successfully.\")\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_product_info(self, product_name: Optional[str] = None, product_id: Optional[int] = None) -> list[dict]:\n",
    "        \"\"\"Gets all product information from the database given the name or ID of the product.\"\"\"\n",
    "        query = \"\"\"SELECT \n",
    "                    product_id,                   \n",
    "                    name,\n",
    "                    inventory,\n",
    "                    price,\n",
    "                    refurbished,\n",
    "                    category\n",
    "                FROM products\n",
    "                WHERE (LOWER(name) = LOWER(%(product_name)s) AND %(product_name)s IS NOT NULL)\n",
    "                   OR (product_id = %(product_id)s AND %(product_id)s IS NOT NULL)\n",
    "                   \"\"\"\n",
    "        if not product_name and not product_id:\n",
    "            print(\"No product name or ID provided.\")\n",
    "            return None\n",
    "        elif product_id:\n",
    "            self.cursor.execute(query, {\"product_name\": None, \"product_id\": product_id})\n",
    "        else:\n",
    "            self.cursor.execute(query, {\"product_name\": product_name, \"product_id\": None})\n",
    "\n",
    "            \n",
    "        rows = self.cursor.fetchall()\n",
    "        columns = [desc[0] for desc in self.cursor.description]\n",
    "        try:\n",
    "            products= DataFrame(rows, columns=columns)\n",
    "            products.to_dict(orient=\"records\")  # <-- JSON serializabl\n",
    "            \n",
    "            return products.to_dict(orient=\"records\")  # <-- JSON serializabl\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching product information: {e}\")\n",
    "            return None\n",
    "    @kernel_function\n",
    "    def most_sold_product(self, category: str) -> Optional[dict]:\n",
    "        \"\"\"Returns the most sold product in a given category.\"\"\"\n",
    "        query = \"\"\"\n",
    "            SELECT products.product_id, products.name, SUM(sales.quantity) AS total_sold\n",
    "            FROM sales\n",
    "            JOIN products ON sales.product_id = products.product_id\n",
    "            WHERE LOWER(products.category) = LOWER(%(category)s) \n",
    "            GROUP BY products.product_id, products.name\n",
    "            ORDER BY total_sold DESC\n",
    "            LIMIT 1;\n",
    "        \"\"\"\n",
    "        self.cursor.execute(query, {\"category\": category})\n",
    "        row = self.cursor.fetchone()\n",
    "        if row:\n",
    "            columns = [desc[0] for desc in self.cursor.description]\n",
    "            return dict(zip(columns, row))\n",
    "        else:\n",
    "            print(f\"No sales data found for category: {category}\")\n",
    "            return None\n",
    "    @kernel_function\n",
    "    async def add_product(self, name: str, description: str, price: float,\n",
    "                           inventory: int, refurbished: bool, category: str ) -> bool:\n",
    "        \"\"\"Adds a new product to the database.\"\"\"\n",
    "        query = \"\"\"INSERT INTO products (name, description, price, inventory, refurbished, category)\n",
    "                   VALUES (%(name)s, %(description)s, %(price)s, %(inventory)s, %(refurbished)s, %(category)s);\"\"\"  \n",
    "        try:\n",
    "            self.cursor.execute(\n",
    "                query,\n",
    "                {\n",
    "                    \"name\": name,\n",
    "                    \"description\": description,\n",
    "                    \"price\": price,\n",
    "                    \"inventory\": inventory,\n",
    "                    \"refurbished\": refurbished,\n",
    "                    \"category\": category\n",
    "                }\n",
    "            )\n",
    "            self.conn.commit()\n",
    "            print(f\"Product '{name}' added successfully.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding product: {e}\")\n",
    "            self.conn.rollback()\n",
    "            return False\n",
    "\n",
    "    def close_connection(self):\n",
    "        \"\"\"Closes the database connection.\"\"\"\n",
    "        self.cursor.close()\n",
    "        self.conn.close()\n",
    "        print(\"Database connection closed.\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da043b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to company's database successfully.\n"
     ]
    }
   ],
   "source": [
    "chat_kernel = Kernel()\n",
    "\n",
    "# Auto-loads defaults from .env file, alternatively you can set endpoint, deployment_name and api_key directly\n",
    "chat_completion = AzureChatCompletion()\n",
    "chat_kernel.add_service(chat_completion)\n",
    "\n",
    "contoso_plugin = Contoso_DataPlugin(db_uri=conn_uri)\n",
    "\n",
    "chat_kernel.add_plugin(\n",
    "    plugin = contoso_plugin,\n",
    "    plugin_name=\"contoso_plugin\"\n",
    ") \n",
    "\n",
    "# Enable planning\n",
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Create a history of the conversation\n",
    "history = ChatHistory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb94c22",
   "metadata": {},
   "source": [
    "#### Example 1: asking a question about product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7015921c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > The product with ID 1 is the \"Macbook Pro M4\". Here are its details:\n",
      "\n",
      "- Category: Laptops\n",
      "- Price: $2085.00\n",
      "- Inventory: 50 units available\n",
      "- Condition: New (not refurbished)\n",
      "\n",
      "If you need more information about this product or others, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Please give me information about the product with id 1\"\n",
    "history.add_user_message(user_message)\n",
    "\n",
    "# Get the response from the AI\n",
    "    \n",
    "result = await chat_completion.get_chat_message_content(\n",
    "    chat_history=history,\n",
    "    settings=execution_settings,\n",
    "    kernel=chat_kernel,\n",
    ")\n",
    "print(\"Assistant > \" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0da4e7",
   "metadata": {},
   "source": [
    "#### Example 2: adding a new product record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b04d7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product 'Lenovo Thinkpad' added successfully.\n",
      "Assistant > The new product \"Lenovo Thinkpad\" (description: Latest model, price: $699.99, inventory: 5, not refurbished, category: Laptops) has been successfully added. If you need more details or further actions, let me know!\n"
     ]
    }
   ],
   "source": [
    "user_message = \"add a new product with name 'Lenovo Thinkpad', description 'Latest model', price 699.99, inventory 5, refurbished False, category 'Laptops'\"\n",
    "history.add_user_message(user_message)\n",
    "result = await chat_completion.get_chat_message_content(\n",
    "    chat_history=history,\n",
    "    settings=execution_settings,\n",
    "    kernel=chat_kernel,\n",
    ")\n",
    "print(\"Assistant > \" + str(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
